---
title: Embedding：推荐系统中从词袋到向量空间的技术跃迁
date: 2025-03-12 17:56:47
tags: LLM
categories: 
- LLM
---

# 1、 基于内容推荐

* 依赖性低，不需要动态的用户行为，只要有内容就可以进行推荐
* 系统不同阶段都可以用：
	* 系统冷启动：内容是任何系统天生的属性，可以从中挖掘特征，实现推荐系统的冷启动。一个复杂的推荐系统都是基于内容推荐成长起来
	* 商品冷启动：不论什么阶段都会有新商品加入，只要商品有内容信息，就可以帮他进行推荐

>数据中台越来越显示其价值
>结构化数据 => 分析、预测、洞察
>非结构化数据 => 检索、推荐、撰写

> 业务驱动数据
> 数据流：企业中的数据中台 => 上层业务分析
> 实现路径：业务驱动，由各个部门、小组发起指定数据（结构化、非结构化）
> 以部门业务为导向，规划结构化数据和非结构化数据


>[!note] 应用形式
> 以智能体展示为主，需要结合员工权限管理和数据安全考虑
> 在用户停留的地方进行集成
> 办公IM：企业微信、钉钉、飞书
> 企业OA、SAP、CRM

>[!note] PMO角色
> AI助手需求收集、推广、运营反馈、开发迭代安排


## 1.1、特征提取和余弦相似

要想实现近似内容推荐首先我们就要理解**特征**和这个**余弦相似度**两个概念
首先余弦相似度就是判断两个向量的余弦值：完全相同时为1，垂直时为0，相反时为 -1 总之相似度是在[-1,1]范围 

![[Pasted image 20250313194407.png]]

![img](20250313194407.png)

![2](20250313134251.png)

![[Pasted image 20250313134831.png]]

![3](20250313134831.png)

上述是一个简单的原理示例就是对于词频TF的一个统计，对于两个句子提取了八个维度的特征（8个词）对于对于八个维度填入每个维度的值（每个词的频率）就形成了一个序列来表达这个句子，这样就可以计算相似度。

但是这样简单的提取维度就会有问题，比如把句子A的“太乱”和“规范”位置换一下，最终得到的序列仍然不变，即使意思已经相反，得到的结果就还是相似。这种可以直接进行比较的方式叫做**词袋模型**

## 1.2 TF-IDF
除了位置之外，特征提取时，某些特征在所有样本均有出现，因此为了尽量忽略这些特征，于是有了**TF-IDF模型**

![[Pasted image 20250313154401.png]]

![4](20250313154401.png)

当一个特征即使频率很高但大家都有时，这个特征的IDF几乎就为0了，不会像之前一样因为频率高而排到前面。


## 1.3 N-Gram
对于位置对意思影响大的问题，又有了N-Gram的词频统计方式，主要是在于分割上

![[Pasted image 20250313140814.png]]

![5](20250313140814.png)

这样去分割会产生更多的维度，自然也能表达更多的信息。假设一句话每个字是一个字母，那么不管多少文本列表，最终的特征也只能取出26个，而如果是二元或者三元，可能产生的特征单位就会很多，N值每提高一个，可能维度都是爆炸式的增长。


## 1.4 案例

> [!capacities-body] 西雅图酒店数据集 
>
> https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Seattle_Hotels.csv
> ![[Pasted image 20250313191444.png]]
>
> ![6](20250313191444.png)
>
> 字段： name,address,desc
>
> 需求：基于用户选择的酒店推荐相似度高的Top10个其他酒店
>
> 方法：计算当前酒店的向量**特征**与整个酒店特征矩阵的**余弦相似度**，取近似度最大的Top-k个
>
> ![[Pasted image 20250313142426.png]]
>
> ![7](20250313142426.png)

 > [!check]+ 代码地址：
 > https://github.com/Jasper-zh/agi_study/tree/main/embeding/feature_extractor




# 2、Embedding

上述使用的用来比较相似的方式不管是词袋还是TF-IDF,都会有一个问题那就是一个维度只表达一个具体的词，那么一个语料库形成的这个表示，维度就会特别高，当对一个新的句子按照这个表进行转化时，很多词可能都没有用过，用很高维来表达这个句子，但是大部分维度都是0，所以这些方式空间利用率太低。

于是就有了Word Embedding 在用一个比较小的维度下尽可能表达出所有词的关系，这样每个维度利用率很高，不会出现之前那种很多维度用不到的情况。

那么实际的工具比如Word2vec
![[Pasted image 20250314143216.png]]

![8](20250314143216.png)

实际上就是找到一个特征空间，满足所有的输入。一开始定好维度，初始化每个词的向量值。

那么核心就是在于如何通过隐藏层，得到每个词的向量表示它们之间相似计算最终都能拟合实际的每个单词和其他单词的上下文关联度。具体就是涉及到优化算法，不停的迭代计算误差不停的梯度下降慢慢这个黑盒子越来越能拟合实际数据，将训练好的内容保存下来即可。

训练好后那么你后续的输入就能通过隐藏层找到对应的向量表示。

那么需要比较的句子就可以通过词嵌入的向量转化成句子的向量表示（简单平均，或者以词嵌入作为输入再去训练一个句子的模型比如RNN、Transformer）

> [!check]+ Word2Vec使用：
 > https://github.com/Jasper-zh/agi_study/tree/main/embeding/word2vec


# 3、总结

上述一开始是没有使用词嵌入来去表达两个句子的相似性，而是选择手动来去提取特征，进行简单的模型计算进行相似度比较。再之后则是通过word embedding的方式对语料库建立了特征空间。从这样一个角度去理解了一个变化过程，再到现在的大语言模型那么它内部的这种向量转化更加复杂，包含的信息量更大，属于模型套模型套n多层，最终通过庞大的语料库训练而成。基本上word embedding这种方式可以用在所有相似推荐的场景，不管是推荐商品还是推荐其他内容，比如商品推荐一个人先访问了什么后访问了什么，就是会形成上下文，这个和文本是一样的（一个文字前面可能是什么字后面可能是什么字）